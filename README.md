# ğŸš€ Retail Sales ETL Pipeline with Data Quality & Analytics

> **Production-Grade Data Engineering Project** | **Python + PostgreSQL + Docker** | **100K+ Records | Zero Defects**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/Python-3.9%2B-blue)](https://www.python.org/)
[![PostgreSQL 15+](https://img.shields.io/badge/PostgreSQL-15%2B-336791)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-Supported-2496ED)](https://www.docker.com/)

---

## ğŸ“‹ Table of Contents

1. [Project Overview](#-project-overview)
2. [Architecture & Data Flow](#-architecture--data-flow)
3. [Project Structure](#-project-structure)
4. [Tech Stack](#-tech-stack)
5. [Quick Start Guide](#-quick-start-guide)
6. [Installation & Setup](#-installation--setup)
7. [Pipeline Execution](#-pipeline-execution)
8. [Data Quality Validation](#-data-quality-validation)
9. [Business Intelligence Outputs](#-business-intelligence-outputs)
10. [Key Metrics](#-key-metrics)
11. [Troubleshooting](#-troubleshooting)
12. [Contributing](#-contributing)

---

## ğŸ¯ Project Overview

This project demonstrates a **production-grade ETL (Extract, Transform, Load) pipeline** that processes **100,000+ retail sales records** through multiple layers of cleaning, validation, and analytics transformation.

### What This Project Does

âœ… **Extracts** raw data from CSV files and APIs  
âœ… **Cleans** messy data (handles duplicates, nulls, data types)  
âœ… **Loads** into PostgreSQL data warehouse  
âœ… **Transforms** raw data into analytics-ready tables  
âœ… **Models** data using Star Schema (Dimensions + Facts)  
âœ… **Validates** data quality with automated checks  
âœ… **Orchestrates** entire pipeline with a single command  
âœ… **Reports** business insights via SQL analytics  

### Why This Matters

- **Real-world problem solving**: Companies don't have clean data
- **Professional infrastructure**: Docker + PostgreSQL + Python
- **Data quality focus**: Automated validation catches errors
- **Business value**: Delivers actionable analytics (KPIs, trends)
- **Scalable design**: Pattern works for 100K rows or 1B rows

---

## ğŸ—ï¸ Architecture & Data Flow

### End-to-End Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        RETAIL ETL DATA PIPELINE                         â”‚
â”‚                    (Production-Grade Architecture)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LAYER 1: DATA INGESTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“¥ RAW DATA SOURCES
  â”œâ”€ CSV Files (105,000 messy records)
  â”œâ”€ API Endpoints (real-time feeds)
  â””â”€ Database snapshots

         â¬‡ï¸ [Python: Pandas, SQLAlchemy]
         
  ğŸ’¾ RAW LAYER (PostgreSQL)
  â””â”€ Table: raw_retail_sales
     â”œâ”€ 105,000 rows (unprocessed)
     â”œâ”€ Contains: duplicates, nulls, wrong types
     â””â”€ Purpose: Source of Truth (immutable archive)


LAYER 2: DATA TRANSFORMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ”§ TRANSFORMATION & CLEANING
  
  DEDUPLICATION
  â””â”€ Remove duplicate transaction_id values
  
  DATA TYPE FIXES
  â”œâ”€ Convert transaction_date to TIMESTAMP
  â”œâ”€ Validate amount > 0 (remove negatives)
  â””â”€ Fix product_category (Unknown â†’ Other)
  
  NULL HANDLING
  â”œâ”€ Remove NULL critical fields
  â”œâ”€ Fill product_category where possible
  â””â”€ Drop rows with missing keys

         â¬‡ï¸ [Result: 98,988 clean rows (6% reduction)]
         
  ğŸ§¹ STAGING LAYER (PostgreSQL)
  â””â”€ Table: stg_retail_sales
     â”œâ”€ 98,988 clean rows (validated)
     â”œâ”€ No nulls, no duplicates, correct types
     â””â”€ Purpose: Clean staging area for analytics


LAYER 3: DATA MODELING (Star Schema)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â­ DIMENSIONAL TABLES (Lookups)
  
  dim_customer (Dimension)
  â”œâ”€ Unique customers from stg_retail_sales
  â”œâ”€ Columns: customer_name, email, email_domain, created_at
  â””â”€ Purpose: Fast customer lookups, segmentation
  
  dim_product_category (Dimension)
  â”œâ”€ Unique product categories
  â”œâ”€ Columns: product_category, is_unknown (data quality flag)
  â””â”€ Purpose: Category analysis, grouping

         â¬‡ï¸ [Join with staging data via keys]
         
  ğŸ”¥ FACT TABLE (Central business events)
  
  fact_sales (Fact - contains business metrics)
  â”œâ”€ 84,077 rows (after dimensional joins)
  â”œâ”€ Columns:
  â”‚  â”œâ”€ transaction_id (business key)
  â”‚  â”œâ”€ transaction_date (time dimension)
  â”‚  â”œâ”€ amount (measure - $$$)
  â”‚  â”œâ”€ customer_email (FK to dim_customer)
  â”‚  â”œâ”€ product_category (FK to dim_product_category)
  â”‚  â””â”€ load_date (data lineage)
  â””â”€ Purpose: Core analytics table (optimized for queries)


LAYER 4: AGGREGATION & KPI CALCULATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“Š PRE-AGGREGATED TABLES (Performance optimization)
  
  agg_daily_category_sales
  â”œâ”€ 190 rows (one per day per category)
  â”œâ”€ Pre-calculated metrics:
  â”‚  â”œâ”€ transaction_date (when)
  â”‚  â”œâ”€ product_category (what)
  â”‚  â”œâ”€ total_sales (KPI: SUM)
  â”‚  â”œâ”€ total_transactions (KPI: COUNT)
  â”‚  â””â”€ avg_transaction_value (KPI: AVG)
  â””â”€ Purpose: Fast BI dashboards (no real-time calculations)


LAYER 5: DATA QUALITY ASSURANCE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… AUTOMATED VALIDATION CHECKS
  
  â”œâ”€ Row Counts
  â”‚  â”œâ”€ stg_retail_sales: 98,988 âœ“
  â”‚  â”œâ”€ fact_sales: 84,077 âœ“
  â”‚  â””â”€ agg_daily_category_sales: 190 âœ“
  â”‚
  â”œâ”€ Null Value Detection
  â”‚  â”œâ”€ Check: transaction_id IS NOT NULL âœ“
  â”‚  â”œâ”€ Check: amount IS NOT NULL âœ“
  â”‚  â””â”€ Check: transaction_date IS NOT NULL âœ“
  â”‚
  â”œâ”€ Business Rule Validation
  â”‚  â”œâ”€ Check: amount > 0 (no negative sales) âœ“
  â”‚  â””â”€ Check: transaction_date within valid range âœ“
  â”‚
  â””â”€ Uniqueness Checks
     â””â”€ Check: No duplicate transaction_id âœ“
  
  ğŸŸ¢ STATUS: ALL CHECKS PASSED (Zero defects!)


LAYER 6: BUSINESS INTELLIGENCE & REPORTING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“ˆ CEO-STYLE INSIGHTS & REPORTS
  
  Business Queries Generated:
  â”œâ”€ Top 5 Product Categories by Revenue
  â”‚  â””â”€ "Toys: $23.3M, Clothing: $23.0M, ..."
  â”‚
  â”œâ”€ Month with Highest Revenue Growth
  â”‚  â””â”€ "February 2026: -77.15% (seasonal decline)"
  â”‚
  â””â”€ Average Transaction Value by Category
     â””â”€ "Clothing: $1,381.54, Toys: $1,368.38, ..."
  
  ğŸ“ Outputs: CSV Reports
  â”œâ”€ Month_with_Highest_Revenue_Growth.csv
  â”œâ”€ Top_5_Product_Categories_by_Sales.csv
  â””â”€ Average_Transaction_Value_per_Category.csv


ORCHESTRATION & AUTOMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ¤– SINGLE COMMAND EXECUTION
  
  $ python scripts/run_pipeline.py
  
  Automatically runs in order:
  1ï¸âƒ£  Load raw data (load.py)
  2ï¸âƒ£  Transform & clean (transform.py)
  3ï¸âƒ£  Build dimensions (build_dimensions.py)
  4ï¸âƒ£  Build fact table (build_fact_sales.py)
  5ï¸âƒ£  Build aggregates (build_aggregates.py)
  6ï¸âƒ£  Quality checks (data_quality_checks.py)
  7ï¸âƒ£  Business reports (business_query_report.py)
  
  âŒ Stops immediately if any step fails (fail-fast pattern)
  âœ… Produces detailed logs for debugging

```

---

## ğŸ“‚ Project Structure

```
retail-etl-pipeline/
â”‚
â”œâ”€â”€ ğŸ“ data/                          # Data storage (raw & processed)
â”‚   â”œâ”€â”€ raw/                          # Original CSV/API files (sample for Git)
â”‚   â”‚   â””â”€â”€ raw_retail_data.csv       # 105,000 messy records
â”‚   â””â”€â”€ processed/                    # Cleaned data cache (not tracked in Git)
â”‚
â”œâ”€â”€ ğŸ³ docker/                        # Docker configuration for PostgreSQL
â”‚   â”œâ”€â”€ Dockerfile                    # (Optional) Custom DB image
â”‚   â””â”€â”€ init.sql                      # Database initialization script
â”‚
â”œâ”€â”€ ğŸ scripts/                       # Core Python ETL code
â”‚   â”‚
â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â””â”€â”€ load.py                   # Step 1: Extract & Load raw data
â”‚   â”‚
â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â””â”€â”€ transform.py              # Step 2: Clean & deduplicate data
â”‚   â”‚
â”‚   â”œâ”€â”€ analytics/
â”‚   â”‚   â”œâ”€â”€ build_dimensions.py       # Step 3: Create dimension tables
â”‚   â”‚   â”œâ”€â”€ build_fact_sales.py       # Step 4: Create fact table
â”‚   â”‚   â”œâ”€â”€ build_aggregates.py       # Step 5: Create KPI aggregates
â”‚   â”‚   â””â”€â”€ business_query_report.py  # Step 7: Generate BI reports
â”‚   â”‚
â”‚   â”œâ”€â”€ quality/
â”‚   â”‚   â””â”€â”€ data_quality_checks.py    # Step 6: Validate data integrity
â”‚   â”‚
â”‚   â”œâ”€â”€ helpers/
â”‚   â”‚   â””â”€â”€ data_generator.py         # Generate fake messy sample data
â”‚   â”‚
â”‚   â””â”€â”€ run_pipeline.py               # ğŸš€ MAIN ORCHESTRATOR (run this!)
â”‚
â”œâ”€â”€ ğŸ“Š sql/                           # SQL scripts for manual exploration
â”‚   â”œâ”€â”€ staging_tables.sql            # Staging layer schema
â”‚   â”œâ”€â”€ marts_tables.sql              # Analytics tables schema
â”‚   â””â”€â”€ queries.sql                   # Useful business queries
â”‚
â”œâ”€â”€ ğŸ§ª tests/                         # Unit tests & validation
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â””â”€â”€ test_load.py
â”‚
â”œâ”€â”€ âš™ï¸ config/                        # Configuration files
â”‚   â””â”€â”€ config.yaml                   # Database config, paths, logging
â”‚
â”œâ”€â”€ ğŸ“‹ reports/                       # Generated BI reports (CSV)
â”‚   â”œâ”€â”€ Month_with_Highest_Revenue_Growth.csv
â”‚   â”œâ”€â”€ Top_5_Product_Categories_by_Sales.csv
â”‚   â””â”€â”€ Average_Transaction_Value_per_Category.csv
â”‚
â”œâ”€â”€ ğŸ” .env                           # Secrets (DO NOT PUSH TO GIT!)
â”‚   â”œâ”€â”€ POSTGRES_USER
â”‚   â”œâ”€â”€ POSTGRES_PASSWORD
â”‚   â”œâ”€â”€ POSTGRES_HOST
â”‚   â”œâ”€â”€ POSTGRES_PORT
â”‚   â””â”€â”€ POSTGRES_DB
â”‚
â”œâ”€â”€ ğŸ“¦ requirements.txt               # Python dependencies
â”‚   â”œâ”€â”€ pandas
â”‚   â”œâ”€â”€ sqlalchemy
â”‚   â”œâ”€â”€ psycopg2-binary
â”‚   â”œâ”€â”€ faker
â”‚   â”œâ”€â”€ python-dotenv
â”‚   â””â”€â”€ pyyaml
â”‚
â”œâ”€â”€ ğŸ‹ docker-compose.yml             # Docker PostgreSQL container orchestration
â”‚
â”œâ”€â”€ ğŸ“„ README.md                      # You are here! ğŸ‘ˆ
â”‚
â”œâ”€â”€ ğŸ“œ LICENSE                        # MIT License (open source)
â”‚
â”œâ”€â”€ ğŸ” .gitignore                     # Git ignore patterns (venv, .env, etc)
â”‚
â””â”€â”€ venv/                             # Python virtual environment (local only)
    â””â”€â”€ [isolated Python packages]

```

### File Descriptions

| Path | Purpose | Owner |
|------|---------|-------|
| `data/raw/` | Original 105K messy records | Pipeline Input |
| `scripts/load/load.py` | Ingests CSV â†’ PostgreSQL | Data Engineer |
| `scripts/transform/transform.py` | Cleans, deduplicates, validates | Data Engineer |
| `scripts/analytics/*.py` | Creates fact/dimension/agg tables | Analytics Engineer |
| `scripts/quality/data_quality_checks.py` | Automated data validation | QA Engineer |
| `scripts/run_pipeline.py` | **Single entry point** â­ | Orchestration |
| `.env` | Database credentials (secrets) | DevOps |
| `docker-compose.yml` | PostgreSQL container config | DevOps |
| `requirements.txt` | Python dependencies | Environment |

---

## ğŸ› ï¸ Tech Stack

### Backend & Data Storage
- **Python 3.9+** - Programming language
- **Pandas** - Data manipulation & cleaning
- **SQLAlchemy** - ORM & database abstraction
- **PostgreSQL 15+** - Relational data warehouse

### Infrastructure & DevOps
- **Docker** - Container for PostgreSQL
- **Docker Compose** - Multi-container orchestration
- **Python venv** - Virtual environment isolation

### Supporting Libraries
- **psycopg2** - PostgreSQL Python driver
- **python-dotenv** - Environment variable management
- **faker** - Synthetic data generation (for testing)
- **pyyaml** - Configuration file parsing

### Development & Operations
- **Git/GitHub** - Version control
- **pgAdmin 4** - PostgreSQL GUI client
- **VS Code** - IDE

---

## ğŸš€ Quick Start Guide

### Prerequisites

Before you begin, ensure you have installed:

- **Python 3.9+** ([Download](https://www.python.org/downloads/))
- **Docker Desktop** ([Download](https://www.docker.com/products/docker-desktop))
- **Git** ([Download](https://git-scm.com/))
- **VS Code** (Optional but recommended)

### 30-Second Setup

```bash
# 1ï¸âƒ£ Clone the repository
git clone https://github.com/SufianNuml/retail-etl-pipeline.git
cd retail-etl-pipeline

# 2ï¸âƒ£ Create & activate virtual environment
python -m venv venv
.\venv\Scripts\Activate.ps1          # Windows PowerShell
# OR
source venv/bin/activate             # macOS/Linux

# 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 4ï¸âƒ£ Start PostgreSQL in Docker
docker compose up -d

# 5ï¸âƒ£ Run the entire pipeline
python scripts/run_pipeline.py

# 6ï¸âƒ£ View results in pgAdmin or CSV reports
# pgAdmin: http://localhost:5050
# Reports: /reports folder
```

---

## ğŸ“– Installation & Setup

### Step 1: Clone Repository

```bash
git clone https://github.com/SufianNuml/retail-etl-pipeline.git
cd retail-etl-pipeline
```

### Step 2: Set Up Virtual Environment

**Why?** Isolates project dependencies from system Python.

```bash
# Windows (PowerShell)
python -m venv venv
.\venv\Scripts\Activate.ps1

# macOS/Linux (Bash/Zsh)
python3 -m venv venv
source venv/bin/activate
```

You should see `(venv)` in your terminal prompt.

### Step 3: Install Python Dependencies

```bash
pip install -r requirements.txt
pip freeze > requirements.txt    # Lock exact versions
```

**What gets installed:**
- `pandas` - Data cleaning & transformation
- `sqlalchemy` - Database connection
- `psycopg2-binary` - PostgreSQL driver
- `faker` - Generate test data
- `python-dotenv` - Load secrets from .env
- `pyyaml` - Read config files

### Step 4: Configure Database Credentials

Create `.env` file in project root (never commit this!):

```env
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_secure_password_here
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=etl_project
```

**âš ï¸ Security:** Add `.env` to `.gitignore` (already done)

### Step 5: Start PostgreSQL with Docker

```bash
# Start container in background
docker compose up -d

# Verify it's running
docker compose ps

# View logs
docker compose logs -f postgres
```

**Check connection:**
```bash
python scripts/verify_db.py
# Output: âœ… PostgreSQL is working! Version info: ...
```

### Step 6: (Optional) Prepare Sample Data

If you don't have `data/raw/raw_retail_data.csv`, generate fake data:

```bash
python scripts/helpers/data_generator.py
# Output: âœ… Generated 105,000 fake retail records
```

---

## ğŸ”„ Pipeline Execution

### Run Full Pipeline (Recommended)

Single command that executes all steps:

```bash
python scripts/run_pipeline.py
```

**Output example:**
```
ğŸ”¥ STARTING FULL RETAIL ETL PIPELINE ğŸ”¥

ğŸš€ Running: scripts/load/load.py
   ğŸ“„ Rows read from CSV: 105000
   âœ… Completed: scripts/load/load.py

ğŸš€ Running: scripts/transform/transform.py
   âœ… Clean rows after transform: 98988
   âœ… Completed: scripts/transform/transform.py

ğŸš€ Running: scripts/analytics/build_fact_sales.py
   âœ… fact_sales rows: 84077
   âœ… Completed: scripts/analytics/build_fact_sales.py

ğŸš€ Running: scripts/analytics/build_aggregates.py
   âœ… agg_daily_category_sales rows: 190
   âœ… Completed: scripts/analytics/build_aggregates.py

ğŸš€ Running: scripts/quality/data_quality_checks.py
   âœ… All quality checks PASSED
   âœ… Completed: scripts/quality/data_quality_checks.py

ğŸ‰ PIPELINE COMPLETED SUCCESSFULLY ğŸ‰
```

### Run Individual Steps (Advanced)

If you only want to run specific parts:

```bash
# Load raw data only
python scripts/load/load.py

# Transform & clean only
python scripts/transform/transform.py

# Build analytics tables
python scripts/analytics/build_dimensions.py
python scripts/analytics/build_fact_sales.py
python scripts/analytics/build_aggregates.py

# Generate BI reports
python scripts/analytics/business_query_report.py

# Run data quality checks
python scripts/quality/data_quality_checks.py
```

### Pipeline Execution Flow Diagram

```
                          START
                            â”‚
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Load Raw Dataâ”‚
                    â”‚ (load.py)     â”‚
                    â”‚ 105K records  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Transform & Clean â”‚
                    â”‚ (transform.py)    â”‚
                    â”‚ Dedup, nulls, etc â”‚
                    â”‚ â†’ 98,988 rows     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Build Dimensions â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚(build_dimensions) â”‚
         â”‚          â”‚ dim_customer      â”‚
         â”‚          â”‚ dim_product_cat   â”‚
         â”‚          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚
    dim_product_category   â”‚
    â”‚                      â–¼
    â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  Build Fact Table â”‚
    â”‚          â”‚(build_fact_sales) â”‚
    â”‚          â”‚ fact_sales: 84K   â”‚
    â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                        â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Build Aggregates  â”‚
                â”‚(build_aggregates) â”‚
                â”‚ agg_daily_sales   â”‚
                â”‚ (190 rows)        â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Quality Checks    â”‚
                â”‚(data_quality_     â”‚
                â”‚ checks.py)        â”‚
                â”‚ âœ… Zero defects   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Generate Reports  â”‚
                â”‚(business_query_   â”‚
                â”‚ report.py)        â”‚
                â”‚ CSV exports       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                    âœ… SUCCESS âœ…
                    
        âŒ ANY STEP FAILS â†’ STOP & EXIT
```

---

## âœ… Data Quality Validation

### Automated Checks

The pipeline includes built-in data quality validation:

```python
# What gets validated:

âœ… Row Counts
   â””â”€ Verify expected number of rows at each layer
   
âœ… Null Value Detection
   â”œâ”€ transaction_id: NOT NULL
   â”œâ”€ amount: NOT NULL
   â””â”€ transaction_date: NOT NULL
   
âœ… Business Rules
   â”œâ”€ amount > 0 (no negative sales)
   â”œâ”€ transaction_date in valid range
   â””â”€ product_category in allowed list
   
âœ… Uniqueness
   â”œâ”€ No duplicate transaction_id
   â””â”€ Unique customer combinations
   
âœ… Data Type Validation
   â”œâ”€ amount: NUMERIC/FLOAT
   â”œâ”€ transaction_date: TIMESTAMP
   â””â”€ customer_name: VARCHAR
```

### Run Quality Checks

```bash
python scripts/quality/data_quality_checks.py
```

**Sample output:**
```
ğŸš¦ STEP 5: DATA QUALITY CHECKS STARTED

ğŸ” Checking row counts...
âœ… stg_retail_sales: 98,988 rows
âœ… fact_sales: 84,077 rows
âœ… agg_daily_category_sales: 190 rows

ğŸ” Checking NULL values in fact_sales...
âœ… No critical NULL values found

ğŸ” Checking negative transaction amounts...
âœ… No negative amounts found (all > $0)

ğŸ” Checking duplicate transactions...
âœ… No duplicate transactions found

ğŸ‰ DATA QUALITY CHECKS COMPLETED - ALL PASSED âœ…
```

---

## ğŸ“Š Business Intelligence Outputs

### Generated Reports

The pipeline produces **CSV reports** that answer business questions:

```bash
python scripts/analytics/business_query_report.py
```

### Report 1: Top Product Categories by Sales

```
product_category    total_sales
Toys                $23,366,508.05
Clothing            $23,044,062.12
Electronics         $22,910,510.77
Home                $22,866,118.94
Other               $22,728,796.66
```

**Business insight:** All categories are roughly balanced (~$23M each) - no single category dominates.

### Report 2: Revenue Growth Trends

```
month          total_sales    prev_month    growth_percent
2026-02-01     $21,373,003.64 $93,542,990   -77.15%
```

**Business insight:** February shows seasonal decline (post-holiday period).

### Report 3: Average Transaction Value (AOV)

```
product_category    avg_transaction_value
Clothing            $1,381.54
Toys                $1,368.38
Home                $1,363.11
Electronics         $1,361.94
Other               $1,359.05
```

**Business insight:** Clothing has highest average order value - premium pricing opportunity.

---

## ğŸ“ˆ Key Metrics

### Data Volume & Performance

| Metric | Value | Status |
|--------|-------|--------|
| **Raw Records Ingested** | 105,000 | âœ… Success |
| **Clean Records (post-transform)** | 98,988 | âœ… 94.2% retention |
| **Fact Table Records** | 84,077 | âœ… Core analytics |
| **Quality Defects Found** | 0 | âœ… Zero defects |
| **Pipeline Execution Time** | ~2-3 seconds | âœ… Fast |

### Data Quality Metrics

| Check | Result | Pass/Fail |
|-------|--------|-----------|
| Null Values in Keys | 0 found | âœ… PASS |
| Duplicate Transactions | 0 found | âœ… PASS |
| Negative Amounts | 0 found | âœ… PASS |
| Invalid Dates | 0 found | âœ… PASS |
| Orphaned Records | 0 found | âœ… PASS |

### Business Metrics

| KPI | Value | Insight |
|-----|-------|---------|
| **Total Revenue** | $113.9M | Strong sales volume |
| **Avg Order Value** | $1,367 | Premium products |
| **Product Categories** | 5 | Diverse portfolio |
| **Date Range** | 60 days | 2 months of data |

---

## ğŸ” Monitoring & Logging

### View Logs

```bash
# Docker PostgreSQL logs
docker compose logs -f postgres

# Pipeline execution logs (saved automatically)
cat logs/pipeline_YYYY-MM-DD.log
```

### Check Database Status

```bash
# Count records in each table
python -c "
import pandas as pd
from sqlalchemy import create_engine
from dotenv import load_dotenv
import os

load_dotenv()
engine = create_engine(
    f'postgresql://...'  # Connection string from .env
)

tables = ['raw_retail_sales', 'stg_retail_sales', 'fact_sales']
for table in tables:
    count = pd.read_sql(f'SELECT COUNT(*) FROM {table}', engine).iloc[0, 0]
    print(f'{table}: {count:,} rows')
"
```

---

## ğŸ› Troubleshooting

### Issue: "ModuleNotFoundError: No module named 'pandas'"

**Solution:** You forgot to activate virtual environment or install dependencies.

```bash
# Activate venv
.\venv\Scripts\Activate.ps1              # Windows
source venv/bin/activate                 # macOS/Linux

# Install dependencies
pip install -r requirements.txt
```

### Issue: "Connection refused: Database not running"

**Solution:** Docker PostgreSQL container is not running.

```bash
# Start container
docker compose up -d

# Check status
docker compose ps

# View logs
docker compose logs postgres
```

### Issue: ".env file not found"

**Solution:** Create `.env` file in project root with database credentials.

```bash
# Create file
echo "POSTGRES_USER=postgres" > .env
echo "POSTGRES_PASSWORD=your_password" >> .env
# ... etc
```

### Issue: "Permission denied: scripts/run_pipeline.py"

**Solution:** Make script executable (macOS/Linux only).

```bash
chmod +x scripts/run_pipeline.py
```

### Issue: "psycopg2: FATAL: password authentication failed"

**Solution:** Check `.env` file has correct PostgreSQL password.

```bash
# Verify credentials match docker-compose.yml
cat .env | grep POSTGRES_PASSWORD
```

---

## ğŸ“ Project Git Commit History

View the development progress:

```bash
git log --oneline

# Example output:
6369c70 feat: implement full ETL orchestration and automated data quality
61cbcb2 Step 3: Transform raw sales data into clean staging table
b18e807 feat: implement load.py and successfully ingest 105k records
736947b feat: setup docker postgres, venv dependencies, and data generator
802271f Initial project structure with folders, scripts, docker, and README
7228b8f Initial commit
```

---

## ğŸ¤ Contributing

Contributions are welcome! To improve this project:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** changes (`git commit -m 'Add amazing feature'`)
4. **Push** to branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

### Development Guidelines

- Follow **PEP 8** Python style
- Add **docstrings** to functions
- Write **unit tests** for new features
- Update **requirements.txt** if adding packages
- Keep **README.md** up to date

---

## ğŸ“ Support & Contact

- **GitHub Issues:** [Open an issue](https://github.com/SufianNuml/retail-etl-pipeline/issues)
- **Email:** sufianaslam127@gmail.com
- **LinkedIn:** [Sufian Numl](https://linkedin.com/in/sufian)

---

## ğŸ“œ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

**In plain English:** You can use, modify, and distribute this code freely. Just give credit! ğŸ˜Š

---

## ğŸ“ Learning Resources

### Understanding ETL Pipelines
- [What is ETL?](https://en.wikipedia.org/wiki/Extract,_transform,_load)
- [Star Schema Data Modeling](https://en.wikipedia.org/wiki/Star_schema)
- [SQL for Data Analysis](https://mode.com/sql-tutorial/)

### Tools & Technologies
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [PostgreSQL Docs](https://www.postgresql.org/docs/)
- [SQLAlchemy ORM](https://docs.sqlalchemy.org/)
- [Docker for Beginners](https://docs.docker.com/get-started/)

### Data Engineering Concepts
- [Data Quality Best Practices](https://www.dataopscentral.com/)
- [Data Warehousing Fundamentals](https://www.ibm.com/topics/data-warehouse)
- [Python for Data Engineering](https://realpython.com/tutorials/data-science/)

---

## ğŸŒŸ Project Highlights

âœ¨ **What Makes This Project Stand Out:**

- **Production-Ready:** Handles 100K+ records with robust error handling
- **Data Quality First:** Automated validation ensures zero defects
- **Star Schema Design:** Professional data warehouse modeling
- **Fully Automated:** Single command runs entire pipeline
- **Well Documented:** README, docstrings, SQL comments everywhere
- **Clean Git History:** Professional commit messages
- **Docker Ready:** No "works on my machine" problems
- **Scalable:** Pattern works from 100K to 1B records

---

## ğŸ‘¨â€ğŸ’¼ About This Project

This project was built to demonstrate **professional-grade data engineering skills** required for companies in **UAE, GCC, and international markets**. It showcases:

âœ… ETL pipeline design  
âœ… Data warehouse modeling  
âœ… Data quality assurance  
âœ… SQL optimization  
âœ… Python programming  
âœ… DevOps infrastructure (Docker)  
âœ… Professional documentation  

**Perfect for:** Portfolio, interviews, learning data engineering!

---

<div align="center">

### â­ If this project helped you, please star it on GitHub! â­

**Made with â¤ï¸ by Sufian | UAE Data Engineer**

</div>

