# ğŸš€ RETAIL ETL PIPELINE - QUICK REFERENCE GUIDE

> **One-page cheat sheet for interviews, troubleshooting, and quick lookup**

---

## âš¡ 30-SECOND PITCH (For Recruiters)

"I built a **production-grade ETL pipeline** that ingests **100,000+ retail sales records**, cleanses messy data (handles duplicates, nulls, bad types), loads into **PostgreSQL**, and transforms it into a **Star Schema data warehouse** with fact & dimension tables. The pipeline includes automated **data quality validation** (zero defects), pre-aggregated KPI tables for fast BI dashboards, and generates **business reports** answering real revenue questions. Everything orchestrates with a **single Python command**. This demonstrates ETL design, SQL optimization, data modeling, Python proficiency, Docker infrastructure, and professional engineering practices."

**Time: 60 seconds** âœ…

---

## ğŸ“Š PROJECT QUICK FACTS

| Aspect | Details |
|--------|---------|
| **Project Name** | Retail Sales ETL Pipeline with Data Quality & Analytics |
| **Data Volume** | 105,000 raw records â†’ 98,988 clean â†’ 84,077 analytics |
| **Tech Stack** | Python, Pandas, SQLAlchemy, PostgreSQL, Docker |
| **Data Quality** | Zero defects (100% pass rate on validation) |
| **Execution Time** | 2-3 seconds (full pipeline) |
| **Schema Pattern** | Star Schema (1 fact + 2 dimension tables) |
| **Key Metrics** | $113.9M revenue, $1,367 avg order value, 5 categories |
| **GitHub** | https://github.com/SufianNuml/retail-etl-pipeline |

---

## ğŸ¯ WHAT THIS PROJECT DEMONSTRATES

### âœ… Data Engineering Skills

- **ETL Design**: Extract â†’ Transform â†’ Load workflow
- **Data Cleaning**: Deduplication, null handling, type conversion
- **Data Modeling**: Star Schema (dimensional modeling)
- **SQL**: Window functions, CTEs, joins, aggregations
- **Python**: Pandas, SQLAlchemy, OOP, automation
- **DevOps**: Docker, containerization, infrastructure
- **Quality Assurance**: Automated validation, testing
- **Orchestration**: Workflow automation, error handling

### âœ… Professional Practices

- Clean code structure (separation of concerns)
- Comprehensive documentation (README, docstrings)
- Version control (Git with meaningful commits)
- Secrets management (.env, .gitignore)
- Logging & monitoring
- Error handling & fail-fast patterns
- Scalable architecture

---

## ğŸ—ï¸ ARCHITECTURE AT A GLANCE

```
RAW DATA (105K) â†’ CLEAN (98K) â†’ STAGE â†’ DIMENSIONS â†’ FACT (84K) â†’ AGGREGATES
       â†“              â†“              â†“        â†“         â†“            â†“
   messy CSV      pandas          stg_     dim_     fact_       agg_daily_
   duplicates     dedup           retail_  customer sales        category
   nulls          validate        sales    dim_prod             _sales
   bad types      fix_types              _cat      KPIs: SUM,
   negatives      standardize            AVG, COUNT

Quality Gate: âœ… Zero Defects â†’ Reports: Revenue, Growth, AOV

```

---

## ğŸ“‚ FILE STRUCTURE

```
scripts/
â”œâ”€â”€ load/load.py                    # Step 1: Load raw data
â”œâ”€â”€ transform/transform.py          # Step 2: Clean & deduplicate
â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ build_dimensions.py         # Step 3: Create dim tables
â”‚   â”œâ”€â”€ build_fact_sales.py         # Step 4: Create fact table
â”‚   â”œâ”€â”€ build_aggregates.py         # Step 5: Create KPI aggregates
â”‚   â””â”€â”€ business_query_report.py    # Step 7: Generate reports
â”œâ”€â”€ quality/data_quality_checks.py  # Step 6: Validate data
â””â”€â”€ run_pipeline.py                 # ğŸš€ MAIN ENTRY POINT

```

---

## ğŸ”„ PIPELINE EXECUTION ORDER

```
1ï¸âƒ£  Load â†’ 2ï¸âƒ£  Transform â†’ 3ï¸âƒ£  Build Dimensions
                                    â†“
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â†“                  â†“
                        dim_customer    dim_product_category
                            â†“                  â†“
                            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                        4ï¸âƒ£  Build Fact Table
                                    â†“
                        5ï¸âƒ£  Build Aggregates
                                    â†“
                        6ï¸âƒ£  Quality Checks
                                    â†“
                        7ï¸âƒ£  Generate Reports
                                    â†“
                            âœ… SUCCESS
```

---

## ğŸš€ QUICK START COMMANDS

```bash
# Setup
git clone https://github.com/SufianNuml/retail-etl-pipeline.git
cd retail-etl-pipeline
python -m venv venv
.\venv\Scripts\Activate.ps1              # Windows
source venv/bin/activate                 # Mac/Linux
pip install -r requirements.txt

# Configure
cp .env.example .env                     # Edit with your credentials
docker compose up -d                     # Start PostgreSQL

# Run
python scripts/run_pipeline.py           # Execute full pipeline

# Verify
python scripts/verify_db.py              # Check database connection
```

---

## ğŸ“Š KEY TABLES

### raw_retail_sales (105K rows)
```
Columns: transaction_id, customer_name, email, amount, product_category, transaction_date
Purpose: Archive all raw data (source of truth)
Quality: Messy (duplicates, nulls, bad types)
```

### stg_retail_sales (98,988 rows)
```
Columns: Same as raw, but cleaned
Purpose: Staging layer for analytics
Quality: Clean (no nulls, no dups, valid types)
```

### dim_customer (~25K rows)
```
Columns: customer_name, email, email_domain, created_at
Purpose: Customer lookup table
Type: Dimension (small, lookup table)
```

### dim_product_category (5 rows)
```
Columns: product_category, is_unknown
Purpose: Category lookup table
Type: Dimension (small, lookup table)
Data Quality: is_unknown flag tracks data issues
```

### fact_sales (84,077 rows)
```
Columns: transaction_id, transaction_date, amount, customer_email, product_category, load_date
Purpose: Core analytics table
Type: Fact (large, contains measures)
Indexes: On transaction_id, transaction_date, customer_email
```

### agg_daily_category_sales (190 rows)
```
Columns: transaction_date, product_category, total_sales, total_transactions, avg_transaction_value
Purpose: Pre-calculated KPIs for dashboards
Type: Aggregate (optimization for BI tools)
Refresh: Daily
```

---

## âœ… DATA QUALITY CHECKS

```
Gate 1: Row Counts        âœ… All tables have expected row counts
Gate 2: Null Detection    âœ… No NULLs in critical columns
Gate 3: Business Rules    âœ… All amounts > 0, dates valid
Gate 4: Uniqueness        âœ… No duplicate transaction_id
Gate 5: Data Types        âœ… All columns correct type

Result: PASSED âœ… (Zero defects)
```

---

## ğŸ“ˆ BUSINESS METRICS

```
Total Revenue:             $113,916,495.98
Number of Transactions:    84,077
Average Order Value (AOV): $1,354.71

By Category:
â”œâ”€ Toys:        $23,366,508 (20.5%)
â”œâ”€ Clothing:    $23,044,062 (20.2%)
â”œâ”€ Electronics: $22,910,510 (20.1%)
â”œâ”€ Home:        $22,866,118 (20.1%)
â””â”€ Other:       $22,728,796 (19.9%)

Growth Trend:
â”œâ”€ January 2026:  $93,542,990
â””â”€ February 2026: $21,373,003 (-77.15% - seasonal)

Highest AOV Category: Clothing ($1,381.54)
```

---

## ğŸ” INTERVIEW Q&A

### Q: Why did you remove 6% of the data during transformation?

**A:** "Of the 105,000 raw records, 6,012 were duplicates (same transaction_id appearing twice - data entry errors). We kept one copy and discarded the duplicates. Additionally, ~1,000 rows had NULL values in critical fields (customer_name, amount). These records are unusable for analytics. The reduction from 105K â†’ 98,988 shows we're strict about data quality - better to have 100% clean data than keep 'bad' rows that would skew analysis."

### Q: What's the Star Schema and why use it?

**A:** "Star Schema has one central FACT table (fact_sales with transactions) surrounded by small DIMENSION tables (customers, products). It's denormalized (redundant data), which is bad for transactions but EXCELLENT for analytics because:
1. Simple queries (fact + 1-2 dimensions, not 5+ table joins)
2. Fast execution (dimensions are tiny, lookups instant)
3. Understandable to BI tools (they're designed for star schemas)

Alternative would be normalized OLTP schema (more tables, complex joins, slower queries)."

### Q: How do you ensure data quality?

**A:** "Automated validation at multiple layers:
1. Load layer: Check row counts match expected
2. Transform layer: Remove nulls, duplicates, invalid values
3. Staging layer: Validate transformed data
4. Quality layer: 5 automated gates (nulls, duplicates, business rules, uniqueness, types)
5. Every gate must PASS or pipeline STOPS (fail-fast)

This ensures dashboards and reports use 100% trusted data."

### Q: Can this scale to billions of rows?

**A:** "Yes! The design doesn't change:
1. Same Python code (Pandas handles chunking)
2. PostgreSQL scales to 100B+ rows
3. Indexes ensure queries stay fast
4. Aggregates reduce query load
5. Partitioning by date divides data naturally

For Petabyte scale, you'd move to Spark/Hadoop, but the CONCEPT is identical."

### Q: What would you change for production?

**A:** "Good question:
1. Add scheduling (Airflow DAG) to run daily
2. Incremental loading (only new records, not full reload)
3. More granular error handling & alerts
4. Data encryption in transit & at rest
5. Backup & disaster recovery strategy
6. Performance monitoring (query logs, slow queries)
7. Multi-environment setup (dev/staging/prod)
8. Data retention policies (archive old data)"

### Q: Most important lesson from this project?

**A:** "Data quality comes FIRST. It's easy to load and transform data; it's hard to know if your data is WRONG. By building quality checks into the pipeline, we catch errors early. A bad data pipeline that nobody trusts is worthless. A clean, validated pipeline is an asset."

---

## ğŸ› ï¸ TROUBLESHOOTING QUICK GUIDE

| Problem | Solution |
|---------|----------|
| **ModuleNotFoundError: pandas** | `pip install -r requirements.txt` |
| **Connection refused (DB)** | `docker compose up -d` to start PostgreSQL |
| **.env not found** | Create .env with `POSTGRES_USER`, `POSTGRES_PASSWORD`, etc |
| **Permission denied** | `chmod +x scripts/run_pipeline.py` (Mac/Linux) |
| **Wrong password (auth failed)** | Check `.env` matches `docker-compose.yml` |
| **Pipeline hangs** | Check Docker: `docker compose ps` |
| **SQL errors** | Verify PostgreSQL is running: `docker compose logs postgres` |

---

## ğŸ’¡ KEY CONCEPTS CHEAT SHEET

### ETL vs ELT
```
ETL (Traditional):    Extract â†’ Transform (in memory) â†’ Load
ELT (Modern):         Extract â†’ Load (to DB) â†’ Transform (SQL)

This project uses: Hybrid (Python transform, then load)
```

### Normalized vs Denormalized
```
Normalized (OLTP):
â”œâ”€ Multiple tables (5+)
â”œâ”€ No data duplication
â”œâ”€ Complex joins
â”œâ”€ Good for: Transactions, updates
â””â”€ Bad for: Analytics queries

Denormalized (OLAP - Star Schema): âœ… USED HERE
â”œâ”€ 1 fact + few dimensions
â”œâ”€ Some data duplication
â”œâ”€ Simple joins
â”œâ”€ Good for: Analytics, queries
â””â”€ Bad for: Transactions, updates
```

### ACID vs BASE
```
ACID (Databases):
â”œâ”€ Atomic (all or nothing)
â”œâ”€ Consistent (valid state)
â”œâ”€ Isolated (concurrent safety)
â””â”€ Durable (survives crashes)

BASE (Data Warehouses):
â”œâ”€ Basic availability
â”œâ”€ Soft state
â”œâ”€ Eventually consistent

This project: ACID (PostgreSQL) âœ…
```

### SUM, COUNT, AVG
```
SUM(amount):     Total revenue        â†’ $113.9M
COUNT(*):        Number of records    â†’ 84,077
AVG(amount):     Average per record   â†’ $1,354.71
MAX(amount):     Highest transaction  â†’ $9,999
MIN(amount):     Lowest transaction   â†’ $1
```

---

## ğŸ“š SQL QUERIES YOU SHOULD KNOW

### Total Revenue by Category
```sql
SELECT product_category, SUM(amount) as revenue
FROM fact_sales
GROUP BY product_category
ORDER BY revenue DESC;
```

### Month-over-Month Growth
```sql
SELECT 
  DATE_TRUNC('month', transaction_date) as month,
  SUM(amount) as revenue,
  LAG(SUM(amount)) OVER (ORDER BY DATE_TRUNC('month', transaction_date)) as prev_month,
  ROUND((SUM(amount) - LAG(SUM(amount)) OVER (...)) / LAG(...) * 100, 2) as growth_pct
FROM fact_sales
GROUP BY month;
```

### Top 5 Customers by Spend
```sql
SELECT customer_email, SUM(amount) as total_spend
FROM fact_sales
GROUP BY customer_email
ORDER BY total_spend DESC
LIMIT 5;
```

### Transactions per Day per Category
```sql
SELECT transaction_date, product_category, COUNT(*) as txn_count
FROM fact_sales
GROUP BY transaction_date, product_category
ORDER BY transaction_date;
```

---

## ğŸ“ LEARNING PATH (What You Mastered)

âœ… **Beginner Skills**
- Python basics (loops, functions, imports)
- CSV file handling (read/write)
- Database connections (SQL basics)

âœ… **Intermediate Skills**
- Pandas (data frames, filtering, transformations)
- SQLAlchemy (ORM, database abstraction)
- SQL (SELECT, JOIN, GROUP BY, aggregation)
- Docker basics (containers, compose)

âœ… **Advanced Skills**
- Star Schema design
- Data quality validation
- ETL orchestration & automation
- Pipeline error handling
- Professional code structure

ğŸš€ **Next Steps**
- Learn Airflow (scheduling pipelines)
- Learn Apache Spark (big data processing)
- Learn cloud platforms (AWS, GCP, Azure)
- Learn BI tools (Tableau, Power BI)

---

## ğŸ“± Interview Confidence Checklist

Before your interview, verify you can explain:

- [ ] What the project does in 60 seconds
- [ ] Why you reduced 105K â†’ 98,988 rows
- [ ] What Star Schema is and why it's useful
- [ ] How you validate data quality
- [ ] What ETL means
- [ ] How the pipeline flows (7 steps)
- [ ] Key metrics and business insights
- [ ] How you'd scale to billions of rows
- [ ] Production considerations (scheduling, incremental, monitoring)
- [ ] What you learned most

---

## ğŸŒŸ ELEVATOR PITCH (For LinkedIn/Resume)

**"Production-grade retail ETL pipeline processing 100K+ records through multi-layer data warehouse. Implemented Star Schema with fact/dimension tables, automated data quality validation (zero defects), and BI-ready aggregates. Stack: Python (Pandas, SQLAlchemy), PostgreSQL, Docker. Demonstrates ETL design, SQL optimization, data modeling, Python proficiency, and engineering best practices."**

**Length:** 50 words  
**Includes:** Technology, scale, outcomes, skills  
**Perfect for:** LinkedIn summary, resume objective, cover letter

---

## ğŸ¯ WHAT TO HIGHLIGHT IN INTERVIEWS

1. **Scale**: 100K+ records (shows can handle data volume)
2. **Quality**: Zero defects (shows attention to detail)
3. **Architecture**: Star Schema (shows database knowledge)
4. **Automation**: Single command (shows engineering maturity)
5. **Professionalism**: Clean code, docs, git history (shows work quality)
6. **Business Value**: Revenue reports, KPIs (shows business sense)

---

## ğŸ”— KEY GITHUB LINKS

```
Repository:    https://github.com/SufianNuml/retail-etl-pipeline
Main script:   scripts/run_pipeline.py
README:        README.md (start here)
Commits:       View git history for development progress
```

---

## ğŸ“ QUICK CONTACT

**Email**: sufianaslam127@gmail.com  
**GitHub**: https://github.com/SufianNuml  
**LinkedIn**: [Add your LinkedIn]

---

<div align="center">

### ğŸ“ YOU ARE NOW A DATA ENGINEER

**With this project, you can confidently apply to:**

âœ… Data Engineer roles  
âœ… ETL Developer positions  
âœ… Analytics Engineer roles  
âœ… Business Intelligence Developer roles  
âœ… Data Analytics positions  

**In companies across UAE, GCC, and internationally.**

### Good luck with your interviews! ğŸš€

</div>

